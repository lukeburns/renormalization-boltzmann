
@article{mehta_exact_2014,
	title = {An exact mapping between the {Variational} {Renormalization} {Group} and {Deep} {Learning}},
	url = {http://arxiv.org/abs/1410.3831},
	abstract = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
	urldate = {2021-12-17},
	journal = {arXiv:1410.3831 [cond-mat, stat]},
	author = {Mehta, Pankaj and Schwab, David J.},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.3831},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
	annote = {Comment: 8 pages, 3 figures},
	file = {arXiv Fulltext PDF:/Users/luke/Zotero/storage/PMX4ZHUA/Mehta and Schwab - 2014 - An exact mapping between the Variational Renormali.pdf:application/pdf;arXiv.org Snapshot:/Users/luke/Zotero/storage/LDJEZPDC/1410.html:text/html},
}

@article{carrasquilla_machine_2017,
	title = {Machine learning phases of matter},
	volume = {13},
	copyright = {2017 Nature Publishing Group},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/nphys4035},
	doi = {10.1038/nphys4035},
	abstract = {The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order.},
	language = {en},
	number = {5},
	urldate = {2021-12-17},
	journal = {Nature Physics},
	author = {Carrasquilla, Juan and Melko, Roger G.},
	month = may,
	year = {2017},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 5
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Phase transitions and critical phenomena;Statistical physics
Subject\_term\_id: phase-transitions-and-critical-phenomena;statistical-physics},
	keywords = {Phase transitions and critical phenomena, Statistical physics},
	pages = {431--434},
	file = {Full Text PDF:/Users/luke/Zotero/storage/YWCD6VZ6/Carrasquilla and Melko - 2017 - Machine learning phases of matter.pdf:application/pdf;Snapshot:/Users/luke/Zotero/storage/PA8RCATD/nphys4035.html:text/html},
}

@article{alterman_boltzmann_2018,
	title = {The {Boltzmann} distribution and the quantum-classical correspondence},
	volume = {51},
	issn = {1751-8113, 1751-8121},
	url = {http://arxiv.org/abs/1710.06051},
	doi = {10.1088/1751-8121/aacf77},
	abstract = {In this paper we explore the following question: can the probabilities constituting the quantum Boltzmann distribution, \$P{\textasciicircum}B\_n {\textbackslash}propto e{\textasciicircum}\{-E\_n/kT\}\$, be derived from a requirement that the quantum configuration-space distribution for a system in thermal equilibrium be very similar to the corresponding classical distribution? It is certainly to be expected that the quantum distribution in configuration space will approach the classical distribution as the temperature approaches infinity, and a well-known equation derived from the Boltzmann distribution shows that this is generically the case. Here we ask whether one can reason in the opposite direction, that is, from quantum-classical agreement to the Boltzmann probabilities. For two of the simple examples we consider---a particle in a one-dimensional box and a simple harmonic oscillator---this approach leads to probability distributions that provably approach the Boltzmann probabilities at high temperature, in the sense that the Kullback-Leibler divergence between the distributions approaches zero.},
	number = {34},
	urldate = {2021-12-17},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Alterman, Sam and Choi, Jaeho and Durst, Rebecca and Fleming, Sarah M. and Wootters, William K.},
	month = aug,
	year = {2018},
	note = {arXiv: 1710.06051},
	keywords = {Quantum Physics},
	pages = {345301},
	annote = {Comment: 12 pages; discussion revised and references added},
	file = {arXiv Fulltext PDF:/Users/luke/Zotero/storage/MVL2ZH5S/Alterman et al. - 2018 - The Boltzmann distribution and the quantum-classic.pdf:application/pdf;arXiv.org Snapshot:/Users/luke/Zotero/storage/BESWGLTB/1710.html:text/html},
}

@article{hashimoto_adscft_2019,
	title = {{AdS}/{CFT} as a deep {Boltzmann} machine},
	volume = {99},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/1903.04951},
	doi = {10.1103/PhysRevD.99.106017},
	abstract = {We provide a deep Boltzmann machine (DBM) for the AdS/CFT correspondence. Under the philosophy that the bulk spacetime is a neural network, we give a dictionary between those, and obtain a restricted DBM as a discretized bulk scalar field theory in curved geometries. The probability distribution as training data is the generating functional of the boundary quantum field theory, and it trains neural network weights which are the metric of the bulk geometry. The deepest layer implements black hole horizons, and an employed regularization for the weights is an Einstein action. A large \$N\_c\$ limit in holography reduces the DBM to a folded feed-forward architecture. We also neurally implement holographic renormalization into an autoencoder. The DBM for the AdS/CFT may serve as a platform for studying mechanisms of spacetime emergence in holography.},
	number = {10},
	urldate = {2021-12-17},
	journal = {Physical Review D},
	author = {Hashimoto, Koji},
	month = may,
	year = {2019},
	note = {arXiv: 1903.04951},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, High Energy Physics - Theory},
	pages = {106017},
	annote = {Comment: 13 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/luke/Zotero/storage/NMHV6KTG/Hashimoto - 2019 - AdSCFT as a deep Boltzmann machine.pdf:application/pdf;arXiv.org Snapshot:/Users/luke/Zotero/storage/K9HAFFVE/1903.html:text/html},
}

@article{zoufal_variational_2021,
	title = {Variational quantum {Boltzmann} machines},
	volume = {3},
	issn = {2524-4914},
	url = {https://doi.org/10.1007/s42484-020-00033-7},
	doi = {10.1007/s42484-020-00033-7},
	abstract = {This work presents a novel realization approach to quantum Boltzmann machines (QBMs). The preparation of the required Gibbs states, as well as the evaluation of the loss function’s analytic gradient, is based on variational quantum imaginary time evolution, a technique that is typically used for ground-state computation. In contrast to existing methods, this implementation facilitates near-term compatible QBM training with gradients of the actual loss function for arbitrary parameterized Hamiltonians which do not necessarily have to be fully visible but may also include hidden units. The variational Gibbs state approximation is demonstrated with numerical simulations and experiments run on real quantum hardware provided by IBM Quantum. Furthermore, we illustrate the application of this variational QBM approach to generative and discriminative learning tasks using numerical simulation.},
	language = {en},
	number = {1},
	urldate = {2021-12-17},
	journal = {Quantum Machine Intelligence},
	author = {Zoufal, Christa and Lucchi, Aurélien and Woerner, Stefan},
	month = feb,
	year = {2021},
	pages = {7},
	file = {Springer Full Text PDF:/Users/luke/Zotero/storage/7PT4RFBM/Zoufal et al. - 2021 - Variational quantum Boltzmann machines.pdf:application/pdf},
}

@article{wilson_renormalization_1974,
	title = {The renormalization group and the $\varepsilon$ expansion},
	volume = {12},
	issn = {03701573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0370157374900234},
	doi = {10.1016/0370-1573(74)90023-4},
	number = {2},
	urldate = {2021-12-17},
	journal = {Physics Reports},
	author = {Wilson, K},
	month = aug,
	year = {1974},
	pages = {75--199},
	file = {Wilson - 1974 - The renormalization group and the ε expansion.pdf:/Users/luke/Zotero/storage/GLW9ZIJZ/Wilson - 1974 - The renormalization group and the ε expansion.pdf:application/pdf},
}

@book{kadanoff_statistical_2000,
	title = {Statistical {Physics}: {Statics}, {Dynamics} and {Renormalization}},
	isbn = {978-981-02-3758-5 978-981-02-4820-8},
	shorttitle = {Statistical {Physics}},
	url = {http://www.worldscientific.com/worldscibooks/10.1142/4016},
	language = {en},
	urldate = {2021-12-17},
	publisher = {WORLD SCIENTIFIC},
	author = {Kadanoff, Leo P},
	month = may,
	year = {2000},
	doi = {10.1142/4016},
	file = {Kadanoff,_Stat.Physics,_Statics,Dynamics&Renormalization.2000.pdf:/Users/luke/Zotero/storage/LBSEK6EK/Kadanoff,_Stat.Physics,_Statics,Dynamics&Renormalization.2000.pdf:application/pdf},
}
